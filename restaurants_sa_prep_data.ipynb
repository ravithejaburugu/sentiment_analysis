{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Bag of words classifiers can work well in longer\n",
    "documents by relying on a few words with strong\n",
    "sentiment like ‘awesome’ or ‘exhilarating.’ However,\n",
    "sentiment accuracies even for binary positive\n",
    "negative classification for single sentences has\n",
    "not exceeded 80% for several years. For the more\n",
    "difficult multiclass case including a neutral class,\n",
    "accuracy is often below 60% for short messages\n",
    "on Twitter\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bag of words classifiers can work well in longer\\ndocuments by relying on a few words with strong\\nsentiment like ‘awesome’ or ‘exhilarating.’ However,\\nsentiment accuracies even for binary positive\\nnegative classification for single sentences has\\nnot exceeded 80% for several years.',\n",
       " 'For the more\\ndifficult multiclass case including a neutral class,\\naccuracy is often below 60% for short messages\\non Twitter']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"very small piece and rice is not at all tasty and i ordered chest piece and i've got leg and no dry fruits qlso. ya very bad experience with this resturant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"very small piece and rice is not at all tasty and i ordered chest piece and i've got leg and no dry fruits qlso.\",\n",
       " 'ya very bad experience with this resturant']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens_list = sent_tokenize(review)\n",
    "sentence_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['very',\n",
       " 'small',\n",
       " 'piece',\n",
       " 'and',\n",
       " 'rice',\n",
       " 'is',\n",
       " 'not',\n",
       " 'at',\n",
       " 'all',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'i',\n",
       " 'ordered',\n",
       " 'chest',\n",
       " 'piece',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'leg',\n",
       " 'and',\n",
       " 'no',\n",
       " 'dry',\n",
       " 'fruits',\n",
       " 'qlso',\n",
       " '.',\n",
       " 'ya',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'experience',\n",
       " 'with',\n",
       " 'this',\n",
       " 'resturant']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(review)\n",
    "print(len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['very',\n",
       " 'small',\n",
       " 'piece',\n",
       " 'and',\n",
       " 'rice',\n",
       " 'is',\n",
       " 'not',\n",
       " 'at',\n",
       " 'all',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'i',\n",
       " 'ordered',\n",
       " 'chest',\n",
       " 'piece',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'got',\n",
       " 'leg',\n",
       " 'and',\n",
       " 'no',\n",
       " 'dry',\n",
       " 'fruits',\n",
       " 'qlso',\n",
       " '.',\n",
       " 'ya',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'experience',\n",
       " 'with',\n",
       " 'this',\n",
       " 'resturant']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpt = nltk.tokenize.WordPunctTokenizer()\n",
    "tokens = wpt.tokenize(review)\n",
    "print(len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['very',\n",
       " 'small',\n",
       " 'piece',\n",
       " 'and',\n",
       " 'rice',\n",
       " 'is',\n",
       " 'not',\n",
       " 'at',\n",
       " 'all',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'i',\n",
       " 'ordered',\n",
       " 'chest',\n",
       " 'piece',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'leg',\n",
       " 'and',\n",
       " 'no',\n",
       " 'dry',\n",
       " 'fruits',\n",
       " 'qlso.',\n",
       " 'ya',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'experience',\n",
       " 'with',\n",
       " 'this',\n",
       " 'resturant']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbwt = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tbwt.tokenize(review)\n",
    "print(len(tokens))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"veri small piec and rice is not at all tasti and i order chest piec and i 've got leg and no dri fruit qlso. ya veri bad experi with thi restur\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(porter_stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"very small piece and rice is not at all tasty and i ordered chest piece and i 've got leg and no dry fruit qlso. ya very bad experience with this resturant\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(wordnet_lemmatizer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.stem.porter.PorterStemmer'> <class 'nltk.stem.porter.PorterStemmer'>\n"
     ]
    }
   ],
   "source": [
    "stemmer_porter = PorterStemmer()\n",
    "print(type(stemmer_porter), type(porter_stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            TOKEN          Porter       Lancaster        Snowball \n",
      "\n",
      "            very            veri            very            veri\n",
      "           small           small            smal           small\n",
      "           piece            piec            piec            piec\n",
      "             and             and             and             and\n",
      "            rice            rice             ric            rice\n",
      "              is              is              is              is\n",
      "             not             not             not             not\n",
      "              at              at              at              at\n",
      "             all             all              al             all\n",
      "           tasty           tasti           tasty           tasti\n",
      "             and             and             and             and\n",
      "               i               i               i               i\n",
      "         ordered           order             ord           order\n",
      "           chest           chest           chest           chest\n",
      "           piece            piec            piec            piec\n",
      "             and             and             and             and\n",
      "               i               i               i               i\n",
      "             've             've             've              ve\n",
      "             got             got             got             got\n",
      "             leg             leg             leg             leg\n",
      "             and             and             and             and\n",
      "              no              no              no              no\n",
      "             dry             dri             dry             dri\n",
      "          fruits           fruit           fruit           fruit\n",
      "           qlso.           qlso.           qlso.           qlso.\n",
      "              ya              ya              ya              ya\n",
      "            very            veri            very            veri\n",
      "             bad             bad             bad             bad\n",
      "      experience          experi          expery          experi\n",
      "            with            with            with            with\n",
      "            this             thi             thi            this\n",
      "       resturant          restur            rest          restur\n"
     ]
    }
   ],
   "source": [
    "stemmers = ['Porter', 'Lancaster', 'Snowball']\n",
    "formatted_row = '{:>16}' * (len(stemmers)+1)\n",
    "print('\\n', formatted_row.format('TOKEN', *stemmers), '\\n')\n",
    "\n",
    "for token in tokens:\n",
    "    stemmed_words = [stemmer_porter.stem(token), stemmer_lancaster.stem(token), stemmer_snowball.stem(token)]\n",
    "    print(formatted_row.format(token, *stemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizers = ['Noun Lemmatizer', 'Verb Lemmatizer']\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                TOKEN     Noun Lemmatizer     Verb Lemmatizer \n",
      "\n",
      "                very                very                very\n",
      "               small               small               small\n",
      "               piece               piece               piece\n",
      "                 and                 and                 and\n",
      "                rice                rice                rice\n",
      "                  is                  is                  be\n",
      "                 not                 not                 not\n",
      "                  at                  at                  at\n",
      "                 all                 all                 all\n",
      "               tasty               tasty               tasty\n",
      "                 and                 and                 and\n",
      "                   i                   i                   i\n",
      "             ordered             ordered               order\n",
      "               chest               chest               chest\n",
      "               piece               piece               piece\n",
      "                 and                 and                 and\n",
      "                   i                   i                   i\n",
      "                 've                 've                 've\n",
      "                 got                 got                 get\n",
      "                 leg                 leg                 leg\n",
      "                 and                 and                 and\n",
      "                  no                  no                  no\n",
      "                 dry                 dry                 dry\n",
      "              fruits               fruit               fruit\n",
      "               qlso.               qlso.               qlso.\n",
      "                  ya                  ya                  ya\n",
      "                very                very                very\n",
      "                 bad                 bad                 bad\n",
      "          experience          experience          experience\n",
      "                with                with                with\n",
      "                this                this                this\n",
      "           resturant           resturant           resturant\n"
     ]
    }
   ],
   "source": [
    "formatted_row = '{:>20}' * (len(lemmatizers)+1)\n",
    "print('\\n', formatted_row.format('TOKEN', *lemmatizers), '\\n')\n",
    "\n",
    "for token in tokens:\n",
    "    lemmatized_words = [wordnet_lemmatizer.lemmatize(token, pos='n'), wordnet_lemmatizer.lemmatize(token, pos='v')]\n",
    "    print(formatted_row.format(token, *lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import names\n",
    "from nltk.classify import accuracy as nltk_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie      like     movie       not\n",
       "0    0.707107  0.000000  0.707107  0.000000\n",
       "1    0.577350  0.000000  0.577350  0.577350\n",
       "2    0.000000  0.707107  0.000000  0.707107\n",
       "3    0.000000  1.000000  0.000000  0.000000\n",
       "4    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "texts = [\n",
    "    \"good movie\", \"not a good movie\", \"did not like\", \n",
    "    \"i like it\", \"good one\"\n",
    "]\n",
    "# using default tokenizer in TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
